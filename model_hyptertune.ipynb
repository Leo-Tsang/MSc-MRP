{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Logistic Regression & RForests\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "#Handle Imbalance Data\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#ML ALgorithims\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leotsang/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#reading our no null master file\n",
    "df = pd.read_csv('df_master.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading our data\n",
    "\n",
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "test = pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['player_name', 'school', 'conference', 'GP', 'Min_per', 'ORtg', 'usg',\n",
       "       'eFG', 'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM',\n",
       "       'FTA', 'FT_per', '2PM', '2PA', '2P_per', '3PM', '3PA', '3P_per',\n",
       "       'blk_per', 'stl_per', 'ftr', 'yr', 'ht', 'num', 'porpag', 'adjoe',\n",
       "       'pfr', 'year', 'pid', 'type', 'rec-rk', 'ast/tov', 'rimmade',\n",
       "       'rimmade + rimmiss', 'midmade', 'midmade + midmiss',\n",
       "       'rimmade/(rimmade+rimmiss)', 'midmade/(midmade+mismiss)', 'dunksmade',\n",
       "       'dunksmiss + dunksmade', 'dunksmade/(dunksmade+dunksmiss)', 'pick',\n",
       "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm',\n",
       "       'mp', 'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk',\n",
       "       'pts', 'pos', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'yr_cat', 'ht_in',\n",
       "       'drafted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats to look at\n",
    "\n",
    "per_stats = ['Min_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FT_per', '2P_per', '3P_per', \n",
    "             'blk_per', 'stl_per', 'GP', 'ht_in', 'yr_cat']\n",
    "box_stats = ['GP', 'mp', 'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg','oreb', 'dreb', 'treb',\n",
    "             'ast', 'stl', 'blk', 'ht_in', 'yr_cat', 'FTA', 'FTM', 'ftr']\n",
    "adv_stats = ['GP', 'mp', 'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'oreb', 'dreb', 'treb',\n",
    "             'ast', 'stl', 'blk', 'ht_in', 'yr_cat', 'bpm', 'obpm', 'dbpm', 'FTA', 'FTM', 'ftr']\n",
    "\n",
    "per_adv = ['Min_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FT_per', '2P_per', '3P_per', \n",
    "             'blk_per', 'stl_per', 'GP', 'ht_in', 'yr_cat', 'bpm', 'obpm', 'dbpm']\n",
    "\n",
    "all_stats = ['GP', 'Min_per', 'ORtg', 'usg',\n",
    "       'eFG', 'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM',\n",
    "       'FTA', 'FT_per', '2PM', '2PA', '2P_per', '3PM', '3PA', '3P_per',\n",
    "       'blk_per', 'stl_per', 'ftr', 'porpag', 'adjoe',\n",
    "       'pfr','year', 'rec-rk', 'ast/tov', \n",
    "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm',\n",
    "       'mp', 'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk',\n",
    "       'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'yr_cat', 'ht_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all missing values\n",
    "train = train.dropna(axis=0, subset=per_stats)\n",
    "train = train.dropna(axis=0, subset=box_stats)\n",
    "train = train.dropna(axis=0, subset=adv_stats)\n",
    "\n",
    "#delete mssing value in test set as well\n",
    "\n",
    "test = test.dropna(axis=0, subset=per_stats)\n",
    "test = test.dropna(axis=0, subset=box_stats)\n",
    "test = test.dropna(axis=0, subset=adv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our 2 test-set aside\n",
    "test_2018 = test[test['year']==2018]\n",
    "test_2019 = test[test['year']==2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SMOTE\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.025)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('over', over), ('under', under),('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "X, y = pipeline.fit_resample(X,y)\n",
    "\n",
    "Counter(X), Counter(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9889666878845745 0.0392156862745098 0.4 5.0\n",
      "0.987481434330575 0.2549019607843137 0.38235294117647056 34.0\n",
      "0.9883301506471462 0.17647058823529413 0.4090909090909091 22.0\n",
      "0.9906641205177169 0.21568627450980393 0.7333333333333333 15.0\n"
     ]
    }
   ],
   "source": [
    "#our preliminary model: Log Reg\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('accuracy: {} recall: {} precision: {} predictions: {}'.format(accuracy, recall, precision, sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.988542329726289 recall: 0.0 precision: 0.0 predictions: 3.0\n",
      "accuracy: 0.9887545088054318 recall: 0.0784313725490196 precision: 0.4 predictions: 10.0\n",
      "accuracy: 0.9896032251220029 recall: 0.1568627450980392 precision: 0.5714285714285714 predictions: 14.0\n",
      "accuracy: 0.9889666878845745 recall: 0.09803921568627451 precision: 0.45454545454545453 predictions: 11.0\n"
     ]
    }
   ],
   "source": [
    "#preliminary Random Forest\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('accuracy: {} recall: {} precision: {} predictions: {}'.format(accuracy, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9866327180140039 recall: 0.19607843137254902 precision: 0.3125 predictions: 32.0\n",
      "accuracy: 0.9879057924888606 recall: 0.27450980392156865 precision: 0.4117647058823529 predictions: 34.0\n",
      "accuracy: 0.9902397623594313 recall: 0.39215686274509803 precision: 0.5714285714285714 predictions: 35.0\n",
      "accuracy: 0.9887545088054318 recall: 0.19607843137254902 precision: 0.45454545454545453 predictions: 22.0\n"
     ]
    }
   ],
   "source": [
    "#preliminary  Adaboost\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = AdaBoostClassifier()\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('accuracy: {} recall: {} precision: {} predictions: {}'.format(accuracy, recall, precision, sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9889666878845745 recall: 0.0392156862745098 precision: 0.4 predictions: 5.0\n",
      "accuracy: 0.9798429874814343 recall: 0.5294117647058824 precision: 0.2755102040816326 predictions: 98.0\n",
      "accuracy: 0.9898154042011458 recall: 0.0784313725490196 precision: 0.8 predictions: 5.0\n",
      "accuracy: 0.9904519414385742 recall: 0.21568627450980393 precision: 0.6875 predictions: 16.0\n"
     ]
    }
   ],
   "source": [
    "###preliminary MLP\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('accuracy: {} recall: {} precision: {} predictions: {}'.format(accuracy, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9872692552514322 recall: 0.3333333333333333 precision: 0.3953488372093023 predictions: 43.0\n",
      "accuracy: 0.9849352853808614 recall: 0.47058823529411764 precision: 0.35294117647058826 predictions: 68.0\n",
      "accuracy: 0.986420538934861 recall: 0.49019607843137253 precision: 0.3968253968253968 predictions: 63.0\n",
      "accuracy: 0.9887545088054318 recall: 0.43137254901960786 precision: 0.4782608695652174 predictions: 46.0\n"
     ]
    }
   ],
   "source": [
    "#with Smote LogREG####\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    oversample = SMOTE(sampling_strategy=0.025)\n",
    "    X, y = oversample.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('accuracy: {} recall: {} precision: {} predictions: {}'.format(accuracy, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9862083598557182 recall: 0.21568627450980393 precision: 0.3055555555555556 predictions: 36.0\n",
      "accuracy: 0.9870570761722894 recall: 0.29411764705882354 precision: 0.375 predictions: 40.0\n",
      "accuracy: 0.9902397623594313 recall: 0.49019607843137253 precision: 0.5555555555555556 predictions: 45.0\n",
      "accuracy: 0.9900275832802886 recall: 0.3333333333333333 precision: 0.5666666666666667 predictions: 30.0\n"
     ]
    }
   ],
   "source": [
    "###WITH SMOTE XGBOOOST#####\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = XGBClassifier(max_depth=4)\n",
    "    oversample = SMOTE(sampling_strategy=0.025)\n",
    "    X, y = oversample.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('Accuracy: {} Recall: {} Precision: {} Pred: {}'.format(accuracy, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9870570761722894 Recall: 0.09803921568627451 Precision: 0.25 Pred: 20.0\n",
      "Accuracy: 0.9900275832802886 Recall: 0.3333333333333333 Precision: 0.5666666666666667 Pred: 30.0\n",
      "Accuracy: 0.9891788669637174 Recall: 0.47058823529411764 Precision: 0.5 Pred: 48.0\n",
      "Accuracy: 0.9811160619562911 Recall: 0.5882352941176471 Precision: 0.30612244897959184 Pred: 98.0\n"
     ]
    }
   ],
   "source": [
    "###WITH SMOTE MLP#####\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    oversample = SMOTE(sampling_strategy=0.025)\n",
    "    X, y = oversample.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('Accuracy: {} Recall: {} Precision: {} Pred: {}'.format(accuracy, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
