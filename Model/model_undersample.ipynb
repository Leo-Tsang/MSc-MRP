{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Logistic Regression & RForests\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "#Handle Imbalance Data\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import smote_variants as sv\n",
    "\n",
    "#ML ALgorithims\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leotsang/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Reading our data\n",
    "\n",
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "test = pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['player_name', 'school', 'conference', 'GP', 'Min_per', 'ORtg', 'usg',\n",
       "       'eFG', 'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM',\n",
       "       'FTA', 'FT_per', '2PM', '2PA', '2P_per', '3PM', '3PA', '3P_per',\n",
       "       'blk_per', 'stl_per', 'ftr', 'yr', 'ht', 'num', 'porpag', 'adjoe',\n",
       "       'pfr', 'year', 'pid', 'type', 'rec-rk', 'ast/tov', 'rimmade',\n",
       "       'rimmade + rimmiss', 'midmade', 'midmade + midmiss',\n",
       "       'rimmade/(rimmade+rimmiss)', 'midmade/(midmade+mismiss)', 'dunksmade',\n",
       "       'dunksmiss + dunksmade', 'dunksmade/(dunksmade+dunksmiss)', 'pick',\n",
       "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm',\n",
       "       'mp', 'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk',\n",
       "       'pts', 'pos', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'ht_in',\n",
       "       'drafted', 'yr_cat', 'ATH', 'GP_adj', 'BBIQ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats to look at\n",
    "\n",
    "per_stats = ['Min_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FT_per', '2P_per', '3P_per', \n",
    "             'blk_per', 'stl_per', 'GP', 'ht_in', 'yr_cat',  'ATH', 'BBIQ']\n",
    "box_stats = ['GP', 'mp', 'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg','oreb', 'dreb', 'treb',\n",
    "             'ast', 'stl', 'blk', 'ht_in', 'yr_cat', 'FTA', 'FTM', 'ftr',  'ATH', 'BBIQ']\n",
    "adv_stats = ['GP', 'mp', 'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'oreb', 'dreb', 'treb',\n",
    "             'ast', 'stl', 'blk', 'ht_in', 'yr_cat', 'bpm', 'obpm', 'dbpm', 'FTA', 'FTM', 'ftr' ,'ATH', 'BBIQ']\n",
    "\n",
    "per_adv = ['Min_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FT_per', '2P_per', '3P_per', \n",
    "             'blk_per', 'stl_per', 'GP', 'ht_in', 'yr_cat', 'bpm', 'obpm', 'dbpm', 'ATH', 'BBIQ']\n",
    "\n",
    "all_stats = ['GP', 'Min_per', 'ORtg', 'usg',\n",
    "       'eFG', 'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM',\n",
    "       'FTA', 'FT_per', '2PM', '2PA', '2P_per', '3PM', '3PA', '3P_per',\n",
    "       'blk_per', 'stl_per', 'ftr', 'porpag', 'adjoe',\n",
    "       'pfr','year', 'rec-rk', 'ast/tov', \n",
    "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm',\n",
    "       'mp', 'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk',\n",
    "       'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'yr_cat', 'ht_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all missing values\n",
    "train = train.dropna(axis=0, subset=per_stats)\n",
    "train = train.dropna(axis=0, subset=box_stats)\n",
    "train = train.dropna(axis=0, subset=adv_stats)\n",
    "\n",
    "#delete mssing value in test set as well\n",
    "\n",
    "test = test.dropna(axis=0, subset=per_stats)\n",
    "test = test.dropna(axis=0, subset=box_stats)\n",
    "test = test.dropna(axis=0, subset=adv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our 2 test-set aside\n",
    "test_2018 = test[test['year']==2018]\n",
    "test_2019 = test[test['year']==2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    390.000000\n",
       "mean       7.188642\n",
       "std        2.765719\n",
       "min       -0.608598\n",
       "25%        5.415707\n",
       "50%        7.235225\n",
       "75%        8.920550\n",
       "max       17.668000\n",
       "Name: bpm, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['drafted']==1]['bpm'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DROP BY BPM\n",
    "\n",
    "train_2 = train[train['bpm']>= -0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3401360544217687 recall: 0.49019607843137253 precision: 0.2604166666666667 predictions: 96.0\n",
      "F1: 0.39080459770114945 recall: 0.6666666666666666 precision: 0.2764227642276423 predictions: 123.0\n",
      "F1: 0.38666666666666666 recall: 0.5686274509803921 precision: 0.29292929292929293 predictions: 99.0\n",
      "F1: 0.4 recall: 0.49019607843137253 precision: 0.33783783783783783 predictions: 74.0\n"
     ]
    }
   ],
   "source": [
    "#with Undersample LogREG####\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    X, y = under.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3404255319148936 recall: 0.47058823529411764 precision: 0.26666666666666666 predictions: 90.0\n",
      "F1: 0.45637583892617445 recall: 0.6666666666666666 precision: 0.3469387755102041 predictions: 98.0\n",
      "F1: 0.45588235294117646 recall: 0.6078431372549019 precision: 0.36470588235294116 predictions: 85.0\n",
      "F1: 0.3968253968253968 recall: 0.49019607843137253 precision: 0.3333333333333333 predictions: 75.0\n"
     ]
    }
   ],
   "source": [
    "###WITH Undersample XGBOOOST#####\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = XGBClassifier(max_depth=4)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    X, y = under.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.18867924528301885 recall: 0.19607843137254902 precision: 0.18181818181818182 predictions: 55.0\n",
      "F1: 0.2368421052631579 recall: 0.17647058823529413 precision: 0.36 predictions: 25.0\n",
      "F1: 0.40944881889763785 recall: 0.5098039215686274 precision: 0.34210526315789475 predictions: 76.0\n",
      "F1: 0.3953488372093023 recall: 0.3333333333333333 precision: 0.4857142857142857 predictions: 35.0\n"
     ]
    }
   ],
   "source": [
    "###WITH Undersample MLP#####\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    X, y = under.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.2893081761006289 recall: 0.45098039215686275 precision: 0.21296296296296297 predictions: 108.0\n",
      "F1: 0.3636363636363636 recall: 0.5490196078431373 precision: 0.27184466019417475 predictions: 103.0\n",
      "F1: 0.4358974358974359 recall: 0.6666666666666666 precision: 0.3238095238095238 predictions: 105.0\n",
      "F1: 0.3401360544217687 recall: 0.49019607843137253 precision: 0.2604166666666667 predictions: 96.0\n"
     ]
    }
   ],
   "source": [
    "###WITH Undersample ADABOOST#####\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    model = AdaBoostClassifier()\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    X, y = under.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.03636363636363636 recall: 0.0196078431372549 precision: 0.25 predictions: 4.0\n",
      "F1: 0.25 recall: 0.19607843137254902 precision: 0.3448275862068966 predictions: 29.0\n",
      "F1: 0.2631578947368421 recall: 0.19607843137254902 precision: 0.4 predictions: 25.0\n",
      "F1: 0.3333333333333333 recall: 0.21568627450980393 precision: 0.7333333333333333 predictions: 15.0\n"
     ]
    }
   ],
   "source": [
    "####WITH UNDERSAMPLE BY BPM\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    #under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    #X, y = under.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(train_2[i], train_2['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.2278481012658228 recall: 0.17647058823529413 precision: 0.32142857142857145 predictions: 28.0\n",
      "F1: 0.3181818181818182 recall: 0.27450980392156865 precision: 0.3783783783783784 predictions: 37.0\n",
      "F1: 0.44680851063829785 recall: 0.4117647058823529 precision: 0.4883720930232558 predictions: 43.0\n",
      "F1: 0.379746835443038 recall: 0.29411764705882354 precision: 0.5357142857142857 predictions: 28.0\n"
     ]
    }
   ],
   "source": [
    "#ADA BOOST Undersample BPM EXP_ID: 8\n",
    "\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = AdaBoostClassifier()\n",
    "    #under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    #X, y = under.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(train_2[i], train_2['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.037037037037037035 recall: 0.0196078431372549 precision: 0.3333333333333333 predictions: 3.0\n",
      "F1: 0.3 recall: 0.23529411764705882 precision: 0.41379310344827586 predictions: 29.0\n",
      "F1: 0.46341463414634143 recall: 0.37254901960784315 precision: 0.6129032258064516 predictions: 31.0\n",
      "F1: 0.1111111111111111 recall: 0.058823529411764705 precision: 1.0 predictions: 3.0\n"
     ]
    }
   ],
   "source": [
    "####WITH UNDERSAMPLE BY BPM\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    #under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    #X, y = under.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(train_2[i], train_2['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.2962962962962963 recall: 0.3137254901960784 precision: 0.2807017543859649 predictions: 57.0\n",
      "F1: 0.47407407407407404 recall: 0.6274509803921569 precision: 0.38095238095238093 predictions: 84.0\n",
      "F1: 0.47540983606557374 recall: 0.5686274509803921 precision: 0.4084507042253521 predictions: 71.0\n",
      "F1: 0.44660194174757284 recall: 0.45098039215686275 precision: 0.4423076923076923 predictions: 52.0\n"
     ]
    }
   ],
   "source": [
    "#XGB UNDER SAMPLE BY BPM EXP_ID: 9\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    under = RandomUnderSampler(sampling_strategy=0.10)\n",
    "    X, y = under.fit_resample(train_2[i], train_2['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.2888888888888889 recall: 0.2549019607843137 precision: 0.3333333333333333 predictions: 39.0\n",
      "F1: 0.4181818181818182 recall: 0.45098039215686275 precision: 0.3898305084745763 predictions: 59.0\n",
      "F1: 0.44660194174757284 recall: 0.45098039215686275 precision: 0.4423076923076923 predictions: 52.0\n",
      "F1: 0.45833333333333326 recall: 0.43137254901960786 precision: 0.4888888888888889 predictions: 45.0\n"
     ]
    }
   ],
   "source": [
    "###WITH SMOTE XGBOOOST##### & UNDERSAMPLING EXP_ID: 10\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = XGBClassifier(max_depth=4)\n",
    "    over = SMOTE(sampling_strategy=0.10)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train_2[i], train_2['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.24175824175824173 recall: 0.43137254901960786 precision: 0.16793893129770993 predictions: 131.0\n",
      "F1: 0.46956521739130436 recall: 0.5294117647058824 precision: 0.421875 predictions: 64.0\n",
      "F1: 0.45544554455445546 recall: 0.45098039215686275 precision: 0.46 predictions: 50.0\n",
      "F1: 0.4444444444444445 recall: 0.39215686274509803 precision: 0.5128205128205128 predictions: 39.0\n"
     ]
    }
   ],
   "source": [
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    over = SMOTE(sampling_strategy=0.10)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train_2[i], train_2['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.2878787878787879 recall: 0.37254901960784315 precision: 0.2345679012345679 predictions: 81.0\n",
      "F1: 0.36231884057971014 recall: 0.49019607843137253 precision: 0.28735632183908044 predictions: 87.0\n",
      "F1: 0.453125 recall: 0.5686274509803921 precision: 0.37662337662337664 predictions: 77.0\n",
      "F1: 0.36507936507936506 recall: 0.45098039215686275 precision: 0.30666666666666664 predictions: 75.0\n"
     ]
    }
   ],
   "source": [
    "for i in tests:\n",
    "    \n",
    "    model = AdaBoostClassifier()\n",
    "    over = SMOTE(sampling_strategy=0.10)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train_2[i], train_2['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.1643835616438356 recall: 0.11764705882352941 precision: 0.2727272727272727 predictions: 22.0\n",
      "F1: 0.3870967741935484 recall: 0.35294117647058826 precision: 0.42857142857142855 predictions: 42.0\n",
      "F1: 0.4000000000000001 recall: 0.35294117647058826 precision: 0.46153846153846156 predictions: 39.0\n",
      "F1: 0.3846153846153846 recall: 0.29411764705882354 precision: 0.5555555555555556 predictions: 27.0\n"
     ]
    }
   ],
   "source": [
    "for i in tests:\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    over = SMOTE(sampling_strategy=0.10)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train_2[i], train_2['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
