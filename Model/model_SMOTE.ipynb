{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Logistic Regression & RForests\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "#Handle Imbalance Data\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import smote_variants as sv\n",
    "\n",
    "#ML ALgorithims\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leotsang/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Reading our data\n",
    "\n",
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "test = pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['player_name', 'school', 'conference', 'GP', 'Min_per', 'ORtg', 'usg',\n",
       "       'eFG', 'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM',\n",
       "       'FTA', 'FT_per', '2PM', '2PA', '2P_per', '3PM', '3PA', '3P_per',\n",
       "       'blk_per', 'stl_per', 'ftr', 'yr', 'ht', 'num', 'porpag', 'adjoe',\n",
       "       'pfr', 'year', 'pid', 'type', 'rec-rk', 'ast/tov', 'rimmade',\n",
       "       'rimmade + rimmiss', 'midmade', 'midmade + midmiss',\n",
       "       'rimmade/(rimmade+rimmiss)', 'midmade/(midmade+mismiss)', 'dunksmade',\n",
       "       'dunksmiss + dunksmade', 'dunksmade/(dunksmade+dunksmiss)', 'pick',\n",
       "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm',\n",
       "       'mp', 'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk',\n",
       "       'pts', 'pos', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'ht_in',\n",
       "       'drafted', 'yr_cat', 'ATH', 'GP_adj', 'BBIQ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats to look at\n",
    "\n",
    "per_stats = ['Min_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FT_per', '2P_per', '3P_per', \n",
    "             'blk_per', 'stl_per', 'GP', 'ht_in', 'yr_cat',  'ATH', 'BBIQ']\n",
    "box_stats = ['GP', 'mp', 'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg','oreb', 'dreb', 'treb',\n",
    "             'ast', 'stl', 'blk', 'ht_in', 'yr_cat', 'FTA', 'FTM', 'ftr',  'ATH', 'BBIQ']\n",
    "adv_stats = ['GP', 'mp', 'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'oreb', 'dreb', 'treb',\n",
    "             'ast', 'stl', 'blk', 'ht_in', 'yr_cat', 'bpm', 'obpm', 'dbpm', 'FTA', 'FTM', 'ftr' ,'ATH', 'BBIQ']\n",
    "\n",
    "per_adv = ['Min_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FT_per', '2P_per', '3P_per', \n",
    "             'blk_per', 'stl_per', 'GP', 'ht_in', 'yr_cat', 'bpm', 'obpm', 'dbpm', 'ATH', 'BBIQ']\n",
    "\n",
    "all_stats = ['GP', 'Min_per', 'ORtg', 'usg',\n",
    "       'eFG', 'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM',\n",
    "       'FTA', 'FT_per', '2PM', '2PA', '2P_per', '3PM', '3PA', '3P_per',\n",
    "       'blk_per', 'stl_per', 'ftr', 'porpag', 'adjoe',\n",
    "       'pfr','year', 'rec-rk', 'ast/tov', \n",
    "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm',\n",
    "       'mp', 'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk',\n",
    "       'pts', '2PA_pg', '2PM_pg', '3PA_pg', '3PM_pg', 'yr_cat', 'ht_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all missing values\n",
    "train = train.dropna(axis=0, subset=per_stats)\n",
    "train = train.dropna(axis=0, subset=box_stats)\n",
    "train = train.dropna(axis=0, subset=adv_stats)\n",
    "\n",
    "#delete mssing value in test set as well\n",
    "\n",
    "test = test.dropna(axis=0, subset=per_stats)\n",
    "test = test.dropna(axis=0, subset=box_stats)\n",
    "test = test.dropna(axis=0, subset=adv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our 2 test-set aside\n",
    "test_2018 = test[test['year']==2018]\n",
    "test_2019 = test[test['year']==2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.07142857142857142 recall: 0.0392156862745098 precision: 0.4 predictions: 5.0\n",
      "F1: 0.30588235294117644 recall: 0.2549019607843137 precision: 0.38235294117647056 predictions: 34.0\n",
      "F1: 0.24657534246575347 recall: 0.17647058823529413 precision: 0.4090909090909091 predictions: 22.0\n",
      "F1: 0.3333333333333333 recall: 0.21568627450980393 precision: 0.7333333333333333 predictions: 15.0\n"
     ]
    }
   ],
   "source": [
    "#our preliminary model: Log Reg\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.0 recall: 0.0 precision: 0.0 predictions: 6.0\n",
      "F1: 0.09836065573770492 recall: 0.058823529411764705 precision: 0.3 predictions: 10.0\n",
      "F1: 0.3384615384615385 recall: 0.21568627450980393 precision: 0.7857142857142857 predictions: 14.0\n",
      "F1: 0.19047619047619047 recall: 0.11764705882352941 precision: 0.5 predictions: 12.0\n"
     ]
    }
   ],
   "source": [
    "#preliminary Random Forest\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.24096385542168675 recall: 0.19607843137254902 precision: 0.3125 predictions: 32.0\n",
      "F1: 0.32941176470588235 recall: 0.27450980392156865 precision: 0.4117647058823529 predictions: 34.0\n",
      "F1: 0.46511627906976744 recall: 0.39215686274509803 precision: 0.5714285714285714 predictions: 35.0\n",
      "F1: 0.273972602739726 recall: 0.19607843137254902 precision: 0.45454545454545453 predictions: 22.0\n"
     ]
    }
   ],
   "source": [
    "#preliminary  Adaboost #EXP_ID 1\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = AdaBoostClassifier()\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.19753086419753085 recall: 0.1568627450980392 precision: 0.26666666666666666 predictions: 30.0\n",
      "F1: 0.21538461538461542 recall: 0.13725490196078433 precision: 0.5 predictions: 14.0\n",
      "F1: 0.2068965517241379 recall: 0.11764705882352941 precision: 0.8571428571428571 predictions: 7.0\n",
      "F1: 0.3614457831325301 recall: 0.29411764705882354 precision: 0.46875 predictions: 32.0\n"
     ]
    }
   ],
   "source": [
    "###preliminary MLP\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.38095238095238093 recall: 0.5490196078431373 precision: 0.2916666666666667 predictions: 96.0\n",
      "F1: 0.3668639053254438 recall: 0.6078431372549019 precision: 0.2627118644067797 predictions: 118.0\n",
      "F1: 0.4246575342465754 recall: 0.6078431372549019 precision: 0.3263157894736842 predictions: 95.0\n",
      "F1: 0.37593984962406013 recall: 0.49019607843137253 precision: 0.3048780487804878 predictions: 82.0\n"
     ]
    }
   ],
   "source": [
    "#with Smote LogREG####\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    oversample = SMOTE(sampling_strategy=0.05)\n",
    "    X, y = oversample.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3269230769230769 recall: 0.3333333333333333 precision: 0.32075471698113206 predictions: 53.0\n",
      "F1: 0.3809523809523809 recall: 0.39215686274509803 precision: 0.37037037037037035 predictions: 54.0\n",
      "F1: 0.48 recall: 0.47058823529411764 precision: 0.4897959183673469 predictions: 49.0\n",
      "F1: 0.44680851063829785 recall: 0.4117647058823529 precision: 0.4883720930232558 predictions: 43.0\n"
     ]
    }
   ],
   "source": [
    "###WITH SMOTE XGBOOOST##### EXP_ID 2\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = XGBClassifier(max_depth=4)\n",
    "    oversample = SMOTE(sampling_strategy=0.05)\n",
    "    X, y = oversample.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.18018018018018017 recall: 0.19607843137254902 precision: 0.16666666666666666 predictions: 60.0\n",
      "F1: 0.27129337539432175 recall: 0.8431372549019608 precision: 0.16165413533834586 predictions: 266.0\n",
      "F1: 0.47619047619047616 recall: 0.39215686274509803 precision: 0.6060606060606061 predictions: 33.0\n",
      "F1: 0.3225806451612903 recall: 0.39215686274509803 precision: 0.273972602739726 predictions: 73.0\n"
     ]
    }
   ],
   "source": [
    "###WITH SMOTE MLP##### EXP_ID: 3\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    oversample = SMOTE(sampling_strategy=0.025)\n",
    "    X, y = oversample.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.2608695652173913 recall: 0.23529411764705882 precision: 0.2926829268292683 predictions: 41.0\n",
      "F1: 0.33333333333333337 recall: 0.29411764705882354 precision: 0.38461538461538464 predictions: 39.0\n",
      "F1: 0.5057471264367817 recall: 0.43137254901960786 precision: 0.6111111111111112 predictions: 36.0\n",
      "F1: 0.30769230769230765 recall: 0.23529411764705882 precision: 0.4444444444444444 predictions: 27.0\n"
     ]
    }
   ],
   "source": [
    "###ADA Boost SMOTE(& NO SMOTE ARE THE SAME) EXP_ID: 4\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = AdaBoostClassifier()\n",
    "    oversample = SMOTE(sampling_strategy=0.4)\n",
    "    X, y = oversample.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(train[i], train['drafted'])\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.27999999999999997 recall: 0.27450980392156865 precision: 0.2857142857142857 predictions: 49.0\n",
      "F1: 0.41441441441441446 recall: 0.45098039215686275 precision: 0.38333333333333336 predictions: 60.0\n",
      "F1: 0.46 recall: 0.45098039215686275 precision: 0.46938775510204084 predictions: 49.0\n",
      "F1: 0.4444444444444445 recall: 0.43137254901960786 precision: 0.4583333333333333 predictions: 48.0\n"
     ]
    }
   ],
   "source": [
    "###WITH SMOTE XGBOOOST##### & UNDERSAMPLING EXP_ID 5\n",
    "\n",
    "tests = [per_stats, box_stats, adv_stats, per_adv] #all_stats]\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = XGBClassifier(max_depth=4)\n",
    "    over = SMOTE(sampling_strategy=0.05)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.25510204081632654 recall: 0.49019607843137253 precision: 0.1724137931034483 predictions: 145.0\n",
      "F1: 0.41624365482233505 recall: 0.803921568627451 precision: 0.2808219178082192 predictions: 146.0\n",
      "F1: 0.42105263157894735 recall: 0.5490196078431373 precision: 0.34146341463414637 predictions: 82.0\n",
      "F1: 0.3354838709677419 recall: 0.5098039215686274 precision: 0.25 predictions: 104.0\n"
     ]
    }
   ],
   "source": [
    "###MLP Under and Over Sample EXP_ID: 6\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    over = SMOTE(sampling_strategy=0.05)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3741935483870968 recall: 0.5686274509803921 precision: 0.27884615384615385 predictions: 104.0\n",
      "F1: 0.38036809815950917 recall: 0.6078431372549019 precision: 0.2767857142857143 predictions: 112.0\n",
      "F1: 0.4217687074829932 recall: 0.6078431372549019 precision: 0.3229166666666667 predictions: 96.0\n",
      "F1: 0.373134328358209 recall: 0.49019607843137253 precision: 0.30120481927710846 predictions: 83.0\n"
     ]
    }
   ],
   "source": [
    "#with Smote LogREG#### & Undersample EXP_ID 7\n",
    "\n",
    "for i in tests:\n",
    "    \n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    over = SMOTE(sampling_strategy=0.05)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train[i], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[i])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    #roc_auc = roc_auc_score(test_2019['drafted'], predictions)\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5084745762711865 recall: 0.5882352941176471 precision: 0.44776119402985076 predictions: 67.0\n",
      "F1: 0.5098039215686274 recall: 0.5098039215686274 precision: 0.5098039215686274 predictions: 51.0\n",
      "F1: 0.4153846153846154 recall: 0.5294117647058824 precision: 0.34177215189873417 predictions: 79.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leotsang/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "####SMOTE & UNDERSAMPLE Experiment 3\n",
    "\n",
    "models = [MLPClassifier(), XGBClassifier(),  LogisticRegression(max_iter=10000)]\n",
    "\n",
    "for i in models:\n",
    "    \n",
    "    model = i\n",
    "    over = SMOTE(sampling_strategy=0.05)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train[adv_stats], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[adv_stats])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.47863247863247865 recall: 0.5490196078431373 precision: 0.42424242424242425 predictions: 66.0\n",
      "F1: 0.48484848484848486 recall: 0.47058823529411764 precision: 0.5 predictions: 48.0\n",
      "F1: 0.4496124031007752 recall: 0.5686274509803921 precision: 0.3717948717948718 predictions: 78.0\n"
     ]
    }
   ],
   "source": [
    "###Expierment 4\n",
    "\n",
    "train_2 = train[(train['bpm']>= -0.6) & (train['pts']>= 3) & (train['ht_in']>= 72)] \n",
    "models = [MLPClassifier(), XGBClassifier(), LogisticRegression(max_iter=10000)]\n",
    "\n",
    "for i in models:\n",
    "    \n",
    "    model = i\n",
    "    over = SMOTE(sampling_strategy=0.1)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "    steps = [('o', over), ('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train_2[adv_stats], train_2['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[adv_stats])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.42592592592592593 recall: 0.45098039215686275 precision: 0.40350877192982454 predictions: 57.0\n",
      "F1: 0.4594594594594595 recall: 0.3333333333333333 precision: 0.7391304347826086 predictions: 23.0\n",
      "F1: 0.4719101123595506 recall: 0.4117647058823529 precision: 0.5526315789473685 predictions: 38.0\n"
     ]
    }
   ],
   "source": [
    "###Expierment 2 Undersmaple by bpm pts, ht_in\n",
    "\n",
    "train_2 = train[(train['bpm']>= -0.6) & (train['pts']>= 3) & (train['ht_in']>= 72)] \n",
    "models = [MLPClassifier(), XGBClassifier(), LogisticRegression(max_iter=10000)]\n",
    "\n",
    "for i in models:\n",
    "    \n",
    "    model = i\n",
    "    #over = SMOTE(sampling_strategy=0.1)\n",
    "    #under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "    #steps = [('o', over), ('u', under)]\n",
    "    #pipeline = Pipeline(steps=steps)\n",
    "    #X, y = pipeline.fit_resample(train[adv_stats], train['drafted'])\n",
    "    model.fit(train_2[adv_stats], train_2['drafted'])\n",
    "    y_pred = model.predict(test_2019[adv_stats])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.39436619718309857 recall: 0.5490196078431373 precision: 0.3076923076923077 predictions: 91.0\n",
      "F1: 0.4347826086956522 recall: 0.5882352941176471 precision: 0.3448275862068966 predictions: 87.0\n",
      "F1: 0.40298507462686567 recall: 0.5294117647058824 precision: 0.3253012048192771 predictions: 83.0\n"
     ]
    }
   ],
   "source": [
    "###Expierment 1 Random Under Sample\n",
    "\n",
    "models = [MLPClassifier(), XGBClassifier(), LogisticRegression(max_iter=10000)]\n",
    "\n",
    "for i in models:\n",
    "    \n",
    "    model = i\n",
    "    #over = SMOTE(sampling_strategy=0.1)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.05)\n",
    "    steps = [('u', under)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train[adv_stats], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[adv_stats])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.41052631578947363 recall: 0.7647058823529411 precision: 0.2805755395683453 predictions: 139.0\n",
      "F1: 0.4603174603174603 recall: 0.5686274509803921 precision: 0.38666666666666666 predictions: 75.0\n",
      "F1: 0.4216216216216216 recall: 0.7647058823529411 precision: 0.291044776119403 predictions: 134.0\n"
     ]
    }
   ],
   "source": [
    "###Experiment 3\n",
    "\n",
    "for i in models:\n",
    "    \n",
    "    model = i\n",
    "    over = SMOTE(sampling_strategy=0.1)\n",
    "    #under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "    steps = [('o', over)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    X, y = pipeline.fit_resample(train[adv_stats], train['drafted'])\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(test_2019[adv_stats])\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    f1 = f1_score(test_2019['drafted'], predictions)\n",
    "    recall = recall_score(test_2019['drafted'], predictions)\n",
    "    precision = precision_score(test_2019['drafted'], predictions)\n",
    "    print('F1: {} recall: {} precision: {} predictions: {}'.format(f1, recall, precision, sum(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
